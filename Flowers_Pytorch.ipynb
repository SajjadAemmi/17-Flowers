{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flowers_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTN54KHiyGHm"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_4d_nVCt2Zd"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9T7le1CtaS4"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "model = torchvision.models.resnet101(pretrained=True)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 17)\n",
        "model = model.to(device)\n",
        "input_size = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEtti4VB7b-6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmFXFZtCxXBN"
      },
      "source": [
        "num_epochs = 16\n",
        "batch_size = 16\n",
        "lr = 0.001\n",
        "\n",
        "TRAIN_DATA_PATH = \"/content/drive/My Drive/datasets/Flowers/Train/\"\n",
        "\n",
        "TRANSFORM_IMG = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(input_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225] )\n",
        "    ])\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=TRANSFORM_IMG)\n",
        "train_data_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True,  num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7NIceGpo0Nf"
      },
      "source": [
        "# Loss and Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E5Om_K5p1ON"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9us4BdIyXHM"
      },
      "source": [
        "# Train the Model\n",
        "for epoch in range(num_epochs):\n",
        "    for (images, labels) in tqdm(train_data_loader):\n",
        "        # Forward + Backward + Optimize\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.data.item()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJG35QmhkQqx"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u256okwKh_m9"
      },
      "source": [
        "batch_size = 8\n",
        "\n",
        "TEST_DATA_PATH = \"/content/drive/My Drive/datasets/Flowers/Test/\"\n",
        "\n",
        "TRANSFORM_IMG = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(input_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225] )\n",
        "    ])\n",
        "\n",
        "test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=TRANSFORM_IMG)\n",
        "test_data_loader  = data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfVdNnTF1_Pq"
      },
      "source": [
        "# Test the Model\n",
        "model.eval()  # Change model to 'eval' mode (BN uses moving mean/var)\n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in tqdm(test_data_loader):\n",
        "\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print('Test Accuracy:', (100.0*correct/total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKPQXIamhgFU"
      },
      "source": [
        "#Save the Trained Model\n",
        "torch.save(model.state_dict(), 'flowers.pth')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}